{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b518cac4-3ef4-4d7b-8ac2-0a27ffbb4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed457341-2d83-4b46-ad01-95b8d5a71d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b63ac08-7b82-44a8-928d-c557b8f8cc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fe78e6-219f-4860-8370-61a7d18bdea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab98ee1-03d9-49ec-b382-ac96056f5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"] = df[\"review\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd2be7c-8692-4aa4-bce9-86fea0756041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8dc53e-e931-4567-8f96-1a4e110f6bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b8117-8445-4aa1-beaa-0d5ec600a0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aa90431-ee15-4d6d-8961-096abd74603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "344a9068-d53b-40de-950c-32127cc8fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4703b29-48d5-41be-9ba1-53703202b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "456d8535-b88f-4b1d-b515-da157f99b5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c35a1fe3-d873-4144-91f4-adc9d49e124a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_html_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226da09-c169-4844-99d3-e1957599cafb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00cee593-bfef-4f44-bb2b-20f0eeb0bf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one of the other reviewers has mentioned that ...\n",
       "1        a wonderful little production. the filming tec...\n",
       "2        i thought this was a wonderful way to spend ti...\n",
       "3        basically there's a family where a little boy ...\n",
       "4        petter mattei's \"love in the time of money\" is...\n",
       "                               ...                        \n",
       "49995    i thought this movie did a down right good job...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997    i am a catholic taught in parochial elementary...\n",
       "49998    i'm going to have to disagree with the previou...\n",
       "49999    no one expects the star trek movies to be high...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.review.apply(remove_html_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8df9934-c302-459c-b0da-99a2f720283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da408cb-240d-4fa4-8e24-7d769537fc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d38c6899-9534-4186-b48d-a6ecffa763bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove url\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad2564bf-2651-40b5-8d87-07aa69c373fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"here is a video about regular expression https://www.youtube.com/results?search_query=regular+expression+campusx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0172ffac-19ea-410f-93c5-4aca499b1d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here is a video about regular expression '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_url(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243eb15f-5bed-4da4-aaa0-9168cae0978c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3cb54454-ade4-4eb4-9058-ee48e46210d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00298b1d-eff1-4756-a115-a35f2655b585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda72e7-f30e-4681-9283-e1dc7c32699d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6908f7ce-d5fe-4364-83b7-faf902930a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0d8fa8b-6410-4937-96d4-f3dac7e6f161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7179d898-1877-47e7-baf6-2573520317cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85038034-f01a-405a-8033-e52c931bbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb55e3b7-3b61-4cf6-a61e-ff951cc48184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "169b7e1b-893d-4a48-bef4-a37f103bccfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"$$#$ hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a43abaea-9250-4e0c-83fa-c6e7f8a30ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "231fe914-9026-4232-ab27-cc99b9dd77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the above function takes time to apply the function on large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e8329e9-a189-4aba-b377-82c43024e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#much faster functions:\n",
    "\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('', '', exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c156384-2376-41c8-9380-37f930b17547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a692e62c-3abe-4147-b107-0ae3715bebf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef9883e1-c2de-4b43-8da1-b83c57f6fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #chat word treatment/ short words\n",
    "\n",
    "# chatwords = [AFAIK=As Far As I Know\n",
    "# AFK=Away From Keyboard\n",
    "# ASAP=As Soon As Possible\n",
    "# ATK=At The Keyboard\n",
    "# ATM=At The Moment\n",
    "# A3=Anytime, Anywhere, Anyplace\n",
    "# BAK=Back At Keyboard\n",
    "# BBL=Be Back Later\n",
    "# BBS=Be Back Soon\n",
    "# BFN=Bye For Now\n",
    "# B4N=Bye For Now\n",
    "# BRB=Be Right Back\n",
    "# BRT=Be Right There\n",
    "# BTW=By The Way\n",
    "# B4=Before\n",
    "# B4N=Bye For Now\n",
    "# CU=See You\n",
    "# CUL8R=See You Later\n",
    "# CYA=See You\n",
    "# FAQ=Frequently Asked Questions\n",
    "# FC=Fingers Crossed\n",
    "# FWIW=For What It's Worth\n",
    "# FYI=For Your Information\n",
    "# GAL=Get A Life\n",
    "# GG=Good Game\n",
    "# GN=Good Night\n",
    "# GMTA=Great Minds Think Alike\n",
    "# GR8=Great!\n",
    "# G9=Genius\n",
    "# IC=I See\n",
    "# ICQ=I Seek you (also a chat program)\n",
    "# ILU=ILU: I Love You\n",
    "# IMHO=In My Honest/Humble Opinion\n",
    "# IMO=In My Opinion\n",
    "# IOW=In Other Words\n",
    "# IRL=In Real Life\n",
    "# KISS=Keep It Simple, Stupid\n",
    "# LDR=Long Distance Relationship\n",
    "# LMAO=Laugh My A.. Off\n",
    "# LOL=Laughing Out Loud\n",
    "# LTNS=Long Time No See\n",
    "# L8R=Later\n",
    "# MTE=My Thoughts Exactly\n",
    "# M8=Mate\n",
    "# NRN=No Reply Necessary\n",
    "# OIC=Oh I See\n",
    "# PITA=Pain In The A..\n",
    "# PRT=Party\n",
    "# PRW=Parents Are Watching\n",
    "# QPSA?\tQue Pasa?\n",
    "# ROFL=Rolling On The Floor Laughing\n",
    "# ROFLOL=Rolling On The Floor Laughing Out Loud\n",
    "# ROTFLMAO=Rolling On The Floor Laughing My A.. Off\n",
    "# SK8=Skate\n",
    "# STATS=Your sex and age\n",
    "# ASL=Age, Sex, Location\n",
    "# THX=Thank You\n",
    "# TTFN=Ta-Ta For Now!\n",
    "# TTYL=Talk To You Later\n",
    "# U=You\n",
    "# U2=You Too\n",
    "# U4E=Yours For Ever\n",
    "# WB=Welcome Back\n",
    "# WTF=What The F...\n",
    "# WTG=Way To Go!\n",
    "# WUF=Where Are You From?\n",
    "# W8=Wait...\n",
    "# 7K=Sick:-D Laugher\n",
    "# TFW â€“ That feeling when. TFW internet slang often goes in a caption to an image.\n",
    "# MFW â€“ My face when\n",
    "# MRW â€“ My reaction when\n",
    "# IFYP â€“ I feel your pain\n",
    "# LOL â€“ Laughing out loud\n",
    "# TNTL â€“ Trying not to laugh\n",
    "# JK â€“ Just kidding\n",
    "# IDC â€“ I donâ€™t care\n",
    "# ILY â€“ I love you\n",
    "# IMU â€“ I miss you\n",
    "# ADIH â€“ Another day in hell\n",
    "# IDC â€“ I donâ€™t care\n",
    "# ZZZ â€“ Sleeping, bored, tired\n",
    "# WYWH â€“ Wish you were here\n",
    "# TIME â€“ Tears in my eyes\n",
    "# BAE â€“ Before anyone else\n",
    "# FIMH â€“ Forever in my heart\n",
    "# BSAAW â€“ Big smile and a wink\n",
    "# BWL â€“ Bursting with laughter\n",
    "# LMAO â€“ Laughing my a** off\n",
    "# BFF: Best friends forever\n",
    "# CSL â€“ Canâ€™t stop laughing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05bc2d4a-4ff7-43ce-be45-66f337a2cd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(user_string):\n",
    "    user_string = user_string.split(\" \")\n",
    "    j = 0\n",
    "    for _str in user_string:\n",
    "        # File path which consists of Abbreviations.\n",
    "        fileName = \"slang.txt\"\n",
    "        # File Access mode [Read Mode]\n",
    "        accessMode = \"r\"\n",
    "        with open(fileName, accessMode) as myCSVfile:\n",
    "            # Reading file as CSV with delimiter as \"=\", so that abbreviation are stored in row[0] and phrases in row[1]\n",
    "            dataFromFile = csv.reader(myCSVfile, delimiter=\"=\")\n",
    "            # Removing Special Characters.\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in dataFromFile:\n",
    "                # Check if selected word matches short forms[LHS] in text file.\n",
    "                if _str.upper() == row[0]:\n",
    "                    # If match found replace it with its appropriate phrase in text file.\n",
    "                    user_string[j] = row[1]\n",
    "            myCSVfile.close()\n",
    "        j = j + 1\n",
    "    # Replacing commas with spaces for final output.\n",
    "    print(' '.join(user_string))\n",
    "    print('===================================================')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67fd92e2-ad3a-429b-aa6f-c2e51b4a1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sentences_to_dict(sentences):\n",
    "    dictionary = {}\n",
    "    for sentence in sentences:\n",
    "        key_value = sentence.split(\"=\")\n",
    "        if len(key_value) == 2:\n",
    "            key, value = key_value\n",
    "            dictionary[key.strip()] = value.strip()\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed8bb6-7cb4-4dbb-8c24-915e7b523892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e781ef-6bae-4adc-ac7e-911c78fe8568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d761b21-244f-4b11-8361-c65a3e9ad5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e76f62af-3821-48ad-bc4b-8167c29be6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spelling correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b51fd73-4793-45ac-b433-d6b6417de7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.8->textblob) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from nltk>=3.8->textblob) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 61.4/626.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 389.1/626.3 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 626.3/626.3 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13344f60-13b2-4d0c-90a3-f88ff2e506c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f3ef5-3a9a-4b86-8bb4-c2d74a6f5630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f9adf-3020-480b-afb7-38c409781a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "794e6744-39b4-459b-9fa3-b8d95cb78896",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = \"ceertain conditionas, during seveal, ggenarations, aree modified in the saame maner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c8fa89f-f02d-4317-82d5-1f60c7f630d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlb = TextBlob(incorrect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7fa0a9e-9a18-4364-8c79-82286a41019f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'certain conditions, during several, generations, are modified in the same manner'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3eabe16-44a4-4da8-8423-2590484318aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d56943d-7859-4973-9b0d-48e7e414ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1de202d0-7a1a-4d04-a796-9343b7b8c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "61ee6544-ed95-413a-a2ba-f967574e9e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for word in text.split():\n",
    "        if word in stopwords:\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e3f54bdb-2092-4c4e-bee3-bccb7190d0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"all my life, all my time favorite movei, a story of the selflessness\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "671e3d21-8571-45f1-95d0-2940c3872b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  life,   time favorite movei,  story   selflessness'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(text = text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0e7d5165-1bf2-4660-b592-cc0b5e3b708c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one    reviewers  mentioned   watching  1 oz e...\n",
       "1         wonderful little production br br  filming te...\n",
       "2         thought    wonderful way  spend time    hot s...\n",
       "3        basically theres  family   little boy jake thi...\n",
       "4        petter matteis love   time  money   visually s...\n",
       "                               ...                        \n",
       "49995     thought  movie    right good job  wasnt  crea...\n",
       "49996    bad plot bad dialogue bad acting idiotic direc...\n",
       "49997       catholic taught  parochial elementary schoo...\n",
       "49998    im going    disagree   previous comment  side ...\n",
       "49999     one expects  star trek movies   high art   fa...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77071711-f0d4-47a5-b136-55898a2ea698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172ae41-48ab-46d8-b18a-ad0b4ef95189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc7d16-7a4d-478c-8fa3-33aaf0d1e23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3a7e5cb4-63f1-412a-ade0-7a5030b5bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34d81791-1331-41d9-aa07-0b0ec92af7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53168bfa-0f3f-4845-9325-001c3e0e8cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff44cd8f-ac87-4451-b9bc-e6f2c0dbd13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loved the movie. It was '"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Loved the movie. It was ðŸ˜˜ðŸ˜˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f4b32178-482f-4ff3-8995-de4a8149e01c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lmao '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_emoji(\"Lmao ðŸ˜‚ðŸ˜‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d6a331df-cd11-4a08-b5d7-58b82bbe4e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.11.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Downloading emoji-2.11.0-py2.py3-none-any.whl (433 kB)\n",
      "   ---------------------------------------- 0.0/433.8 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/433.8 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 81.9/433.8 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  430.1/433.8 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 433.8/433.8 kB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa535dae-7feb-40bb-961a-2434cb845b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :fire:\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "115bec4e-7f16-4707-b6fd-d3874a456c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n"
     ]
    }
   ],
   "source": [
    "print(emoji.demojize('Loved the movie. It was ðŸ˜˜'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5324f67-bf67-4a2d-a497-765c24074dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7861bc7-2283-47ce-85f3-ab4493925bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d68b45-ea55-4a7c-ad00-055267aa2e61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3a3b0e81-8d6a-4294-92cc-950f1f9b2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing, TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e483830a-1025-421e-921b-50bf578df102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "97fb8cf5-3ebf-48e5-ba98-5b6fa8c0e699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = \"I am going to delhi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f2f338ad-5f1b-45af-a37f-f2286cdb3b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8d37d168-1be4-4377-9286-f942a8863ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = \"i am going to delhi. I will stay there for 3 days. lets hope the trip to be great.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "678c5eb4-fb4c-4810-99f5-de2f6bff36e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " ' lets hope the trip to be great',\n",
       " '']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d3e189-1a42-4060-99d4-b9681c29e18d",
   "metadata": {},
   "source": [
    "#problems with split function\n",
    "\n",
    "cannot split on dfferent punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "14882d9b-57db-407a-aede-f8fef4cc8e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent3 = \"i am going to delhi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e99bca3f-e582-42e6-839c-3621e6f49de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4 = 'where do think i should go? i have 3 day holiday'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf13c10f-640c-469c-926d-e0c3bdeec999",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c5b06-dd03-45a5-b74e-5879a3ae6fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1671be06-a576-4616-b826-a4b33aabc2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e0c95ba3-a6cb-4dfb-9a89-84ef03ca9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3cff93ec-8707-4779-8b63-8e5fac989224",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall(\"[\\w']+\", sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b6185e6-8c2f-440d-9baf-cc01b73b0fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89f299c0-c474-483c-a523-60308802276a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \"\\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e127654-c09b-4c28-a81a-1c0f326b915b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f99c3814-34db-4836-bd46-a8a3c897007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ef154ce9-51dc-48d6-90b5-a619ffe1d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "06e5dfee-cbd9-4be5-85da-d0425b677dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'going', 'to', 'delhi', '!']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5097ef-1b8e-49c8-80d8-e3175094f3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "799740f0-194c-4231-8b84-2b7622254cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7098c1d7-db6d-4863-80e7-d0e0b16ce09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "07f7f63e-3f7d-4b2b-a4b7-c28114385533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4f8d5dbd-047c-4168-b4a9-7d7104e4a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b36c9e26-3257-40cf-af4c-3bb95de99f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f5c31-6e6a-4403-9ac2-99d755aa5063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8c2d17cc-952a-4771-9213-6707a16991a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.4.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.3.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
      "     ---------------------------------------- 0.0/85.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.1/85.1 kB 5.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (24.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.16.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp310-cp310-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.4-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<0.17.0,>=0.7.0 (from weasel<0.4.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prajw\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.7.4-cp310-cp310-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/12.1 MB 26.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 1.6/12.1 MB 20.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 2.0/12.1 MB 15.6 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 2.3/12.1 MB 13.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.5/12.1 MB 11.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.8/12.1 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.2/12.1 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.6/12.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.0/12.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 4.2/12.1 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.5/12.1 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.8/12.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 5.1/12.1 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.3/12.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.4/12.1 MB 7.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.5/12.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 6.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.7/12.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.8/12.1 MB 5.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 5.9/12.1 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 6.0/12.1 MB 5.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.1/12.1 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 6.2/12.1 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.4/12.1 MB 5.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.6/12.1 MB 5.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.8/12.1 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.9/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.1/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.4/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.6/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.8/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 8.1/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.6/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.9/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.2/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.6/12.1 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.9/12.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.3/12.1 MB 5.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 10.6/12.1 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.4/12.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.8/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 181.6/181.6 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
      "   ---------------------------------------- 0.0/394.9 kB ? eta -:--:--\n",
      "   ------------------------------------ -- 368.6/394.9 kB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 394.9/394.9 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.16.3-cp310-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/1.9 MB 8.3 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.9/1.9 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.3/1.9 MB 10.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.7/1.9 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 8.5 MB/s eta 0:00:00\n",
      "Downloading smart_open-6.4.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.0/57.0 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "   ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "   --------------------------------------  481.3/481.9 kB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 481.9/481.9 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.3-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.4/1.5 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.7/1.5 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading typer-0.9.4-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/46.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 46.0/46.0 kB ? eta 0:00:00\n",
      "Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.3.4-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB ? eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading blis-0.7.11-cp310-cp310-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.4/6.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.9/6.6 MB 11.8 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.4/6.6 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.8/6.6 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.5/6.6 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.7/6.6 MB 8.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 2.9/6.6 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 3.1/6.6 MB 7.6 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 3.3/6.6 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.4/6.6 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 3.7/6.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.1/6.6 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 4.6/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.2/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 6.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.4/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 6.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.16.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.0/45.0 kB ? eta 0:00:00\n",
      "Downloading confection-0.1.4-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, pydantic-core, murmurhash, langcodes, cloudpathlib, catalogue, blis, annotated-types, typer, srsly, pydantic, preshed, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.6.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.16.0 confection-0.1.4 cymem-2.0.8 langcodes-3.3.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.6.4 pydantic-core-2.16.3 smart-open-6.4.0 spacy-3.7.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.3 typer-0.9.4 wasabi-1.1.2 weasel-0.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d953f4b-154c-4104-881f-b8836cbdba30",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[106], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_sm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4382f3a3-be43-4d4c-9e1e-61dfe80a0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805369a-dd04-4101-9af3-3412e6bbc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cf2a8a3e-a033-47a9-8bc1-c1095fd599ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e27f0b3a-fb3b-4ce3-870b-b392d1cce41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a4231722-ee7e-4b79-b6ab-a0d3bb167cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f2a175a3-003c-417a-8c76-3ff79fdbe501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "657811ec-7bda-43d7-ab51-17d3755593e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"walk walks walking walked\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3bb77433-ee35-4b40-98e4-6d31f7814c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429bdc02-ff9e-479c-af57-c93b8f3da82f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "045677b9-9317-45ea-962c-8393c23a1386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "text = 'probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie'\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e3a53540-44c6-4a8a-a9e1-e2b1091e49c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671367c-3661-438b-b8f8-2788160eb92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07636835-9138-435b-b19b-508b466633e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "\n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "88ddc4b4-88fc-41df-ac23-98713999fdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prajw\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19134146-687d-4c79-a9c3-7f464be61c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7f066-acf3-4961-9731-6e851581f96d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4da7fc-5340-4d2e-bb6b-9f56858d208c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46b62bb-c552-452b-b73a-1d8f201d30ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a8abc9-56b7-408a-9e35-6f360d9dc315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b3480-c242-479f-a29b-647c77fd3cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a742973-2bd6-4acf-80b4-cbe933d61eb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456020c1-4680-4215-b56a-031bc32e74a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc1d7d-c4c1-42a3-9690-d9b4c2c9a73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a935bd-72cc-4b60-b14c-52c19815478d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa754c-3e91-42b1-8e74-ceb54c8b3a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac498a81-fc85-4e44-85d2-6ae1c5e2efba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
